---
id: sensor-simulation
title: Sensor Simulation
slug: /module-02-simulation/03-sensor-simulation
---

Accurate sensor data is paramount for any autonomous robot. In a digital twin, simulating sensors allows us to develop and test perception algorithms without requiring physical hardware. Gazebo provides robust capabilities for emulating various types of sensors.

## Why Sensor Simulation?

*   **Cost-Effective**: Avoids the expense and maintenance of physical sensors during early development.
*   **Safety**: Allows testing of hazardous scenarios without risk to humans or hardware.
*   **Repeatability**: Ensures experiments can be replicated exactly, aiding in debugging and performance evaluation.
*   **Accessibility**: Enables development for robots with unique or custom sensor configurations.

## Common Simulated Sensors in Gazebo

Gazebo supports the simulation of a wide range of sensors, including:

*   **Cameras**: RGB, depth, and stereo cameras, providing realistic image and depth data.
*   **LiDAR (Laser Scanners)**: Simulate 2D and 3D laser scans for mapping and navigation.
*   **IMU (Inertial Measurement Unit)**: Provides angular velocity, linear acceleration, and orientation data.
*   **Contact Sensors**: Detect physical contact between robot parts and the environment.
*   **GPS**: Simulate global positioning data.

### Camera Sensor Example (SDF snippet)

Camera sensors are defined within a robot's SDF model. You specify parameters like resolution, field of view, update rate, and the output topic name.

```xml
<sensor name="camera" type="camera">
  <camera>
    <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>
      <far>100</far>
    </clip>
  </camera>
  <always_on>1</always_on>
  <update_rate>30.0</update_rate>
  <visualize>true</visualize>
  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
    <ros> <!-- ROS 2 plugin configuration -->
      <namespace>/my_robot</namespace>
      <point_cloud_cutoff>0.5</point_cloud_cutoff>
      <frame_name>camera_link_optical</frame_name>
      <depth_image_topic_name>depth/image_raw</depth_image_topic_name>
      <depth_image_camer-info_topic_name>depth/camer-info</depth_image_camer-info_topic_name>
      <point_cloud_topic_name>depth/points</point_cloud_topic_name>
      <publish_tf>1</publish_tf>
    </ros>
  </plugin>
</sensor>
```

## Simulating Sensor Noise

Realistic simulations often require modeling sensor noise. Gazebo allows you to add various noise parameters to sensors (e.g., Gaussian noise for cameras or IMUs) to better reflect real-world sensor behavior. This is crucial for developing robust perception algorithms that can handle imperfections in sensor data.

```xml
<!-- Example: Gaussian noise for an IMU sensor -->
<sensor name="imu_sensor" type="imu">
  <imu>
    <orientation>
      <x><noise type="gaussian"><mean>0.0</mean><stddev>2e-4</stddev><bias_mean>0.0</bias_mean><bias_stddev>7.5e-5</bias_stddev></noise></x>
      <y><noise type="gaussian"><mean>0.0</mean><stddev>2e-4</stddev><bias_mean>0.0</bias_mean><bias_stddev>7.5e-5</bias_stddev></noise></y>
      <z><noise type="gaussian"><mean>0.0</mean><stddev>2e-4</stddev><bias_mean>0.0</bias_mean><bias_stddev>7.5e-5</bias_stddev></noise></z>
    </orientation>
    <!-- ... other IMU noise parameters ... -->
  </imu>
</sensor>
```

By accurately simulating sensor data and noise, you can effectively develop and validate perception and navigation algorithms for your humanoid robots within the digital twin environment.
