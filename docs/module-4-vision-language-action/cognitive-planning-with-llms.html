<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-4-vision-language-action/cognitive-planning-with-llms" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Cognitive Planning with LLMs | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/img/book_cover.png"><meta data-rh="true" name="twitter:image" content="https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/img/book_cover.png"><meta data-rh="true" property="og:url" content="https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Cognitive Planning with LLMs | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Imagine telling a robot, &quot;Please clean the room,&quot; and it understands, plans, and executes the necessary steps. This is the promise of Cognitive Planning with Large Language Models (LLMs). It bridges the vast gap between high-level human intent expressed in natural language and the precise, atomic actions a robot must perform. LLMs are not just for generating text; they are becoming powerful reasoning engines for robotic control."><meta data-rh="true" property="og:description" content="Imagine telling a robot, &quot;Please clean the room,&quot; and it understands, plans, and executes the necessary steps. This is the promise of Cognitive Planning with Large Language Models (LLMs). It bridges the vast gap between high-level human intent expressed in natural language and the precise, atomic actions a robot must perform. LLMs are not just for generating text; they are becoming powerful reasoning engines for robotic control."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Course/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms"><link data-rh="true" rel="alternate" href="https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms" hreflang="en"><link data-rh="true" rel="alternate" href="https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Cognitive Planning with LLMs","item":"https://mhammadkhan17.github.io/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms"}]}</script><link rel="alternate" type="application/rss+xml" href="/Physical-AI-Humanoid-Robotics-Course/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/Physical-AI-Humanoid-Robotics-Course/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Course/assets/css/styles.6f4f9ba0.css">
<script src="/Physical-AI-Humanoid-Robotics-Course/assets/js/runtime~main.98c04993.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Course/assets/js/main.c1117887.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Physical-AI-Humanoid-Robotics-Course/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Course/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Course/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Course/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-1-robotic-nervous-system/ros-nodes-topics-services">Documentation</a><a class="navbar__item navbar__link" href="/Physical-AI-Humanoid-Robotics-Course/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Mhammadkhan17/Physical-AI-Humanoid-Robotics-Course" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-1-robotic-nervous-system/ros-nodes-topics-services"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulating-physics-in-gazebo"><span title="Module 2: The Digital Twin (Gazebo &amp; Unity)" class="categoryLinkLabel_W154">Module 2: The Digital Twin (Gazebo &amp; Unity)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-robot-brain/nvidia-isaac-sim"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac™)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac™)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/voice-to-action-whisper"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/voice-to-action-whisper"><span title="Voice-to-Action with Whisper" class="linkLabel_WmDU">Voice-to-Action with Whisper</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms"><span title="Cognitive Planning with LLMs" class="linkLabel_WmDU">Cognitive Planning with LLMs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/capstone-project"><span title="Capstone Project: The Autonomous Humanoid" class="linkLabel_WmDU">Capstone Project: The Autonomous Humanoid</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Course/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: Vision-Language-Action (VLA)</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Cognitive Planning with LLMs</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Cognitive Planning with LLMs</h1></header><p>Imagine telling a robot, &quot;Please clean the room,&quot; and it understands, plans, and executes the necessary steps. This is the promise of <strong>Cognitive Planning with Large Language Models (LLMs)</strong>. It bridges the vast gap between high-level human intent expressed in natural language and the precise, atomic actions a robot must perform. LLMs are not just for generating text; they are becoming powerful reasoning engines for robotic control.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-role-of-llms-in-robotic-planning">The Role of LLMs in Robotic Planning<a href="#the-role-of-llms-in-robotic-planning" class="hash-link" aria-label="Direct link to The Role of LLMs in Robotic Planning" title="Direct link to The Role of LLMs in Robotic Planning" translate="no">​</a></h2>
<p>Traditional robotic planning often relies on predefined state machines or symbolic planning languages (like PDDL). While effective for constrained problems, they struggle with the ambiguity and vastness of real-world human commands. LLMs, with their immense pre-trained knowledge, offer a new paradigm:</p>
<ul>
<li class=""><strong>Semantic Understanding</strong>: LLMs excel at interpreting complex, nuanced natural language instructions, handling synonyms, implied meanings, and contextual cues.</li>
<li class=""><strong>Task Decomposition</strong>: They can break down a high-level goal (e.g., &quot;make coffee&quot;) into a logical sequence of sub-tasks (e.g., &quot;get mug,&quot; &quot;fill with water,&quot; &quot;insert pod&quot;).</li>
<li class=""><strong>Common Sense Reasoning</strong>: LLMs possess a vast amount of implicit world knowledge. They understand that &quot;washing dishes&quot; involves water and soap, or that a &quot;cup&quot; needs to be upright to be filled. This common sense helps in generating more robust and realistic plans.</li>
<li class=""><strong>Adaptability</strong>: With proper prompting, LLMs can adapt plans based on dynamic environmental factors or robot capabilities.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-cognitive-planning-pipeline-llm--ros-2">The Cognitive Planning Pipeline (LLM + ROS 2)<a href="#the-cognitive-planning-pipeline-llm--ros-2" class="hash-link" aria-label="Direct link to The Cognitive Planning Pipeline (LLM + ROS 2)" title="Direct link to The Cognitive Planning Pipeline (LLM + ROS 2)" translate="no">​</a></h2>
<p>Connecting an LLM to a robot in a practical way involves a carefully orchestrated pipeline within the ROS 2 framework.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-input-high-level-natural-language-command">1. Input: High-Level Natural Language Command<a href="#1-input-high-level-natural-language-command" class="hash-link" aria-label="Direct link to 1. Input: High-Level Natural Language Command" title="Direct link to 1. Input: High-Level Natural Language Command" translate="no">​</a></h3>
<p>This command typically comes from a Speech-to-Text system (like the Whisper node discussed in the previous chapter) and is published to a ROS 2 topic (e.g., <code>/voice_commands/text</code>). Example: <code>&quot;Robot, could you please organize my desk?&quot;</code></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-llm-reasoning-node-the-planner">2. LLM &quot;Reasoning&quot; Node (The Planner)<a href="#2-llm-reasoning-node-the-planner" class="hash-link" aria-label="Direct link to 2. LLM &quot;Reasoning&quot; Node (The Planner)" title="Direct link to 2. LLM &quot;Reasoning&quot; Node (The Planner)" translate="no">​</a></h3>
<p>This is a dedicated ROS 2 node that acts as the interface to the LLM API (e.g., OpenAI&#x27;s GPT models, Google&#x27;s Gemini, or local models).</p>
<ul>
<li class="">
<p><strong>Prompt Engineering</strong>: This is the most critical component. The prompt provided to the LLM instructs it on its role and expected output. It should include:</p>
<ul>
<li class=""><strong>Role Definition</strong>: &quot;You are a helpful robot planning assistant...&quot;</li>
<li class=""><strong>Goal</strong>: The natural language command received from the user.</li>
<li class=""><strong>Available Actions</strong>: A list of atomic, executable robot actions with their parameters (e.g., <code>move_to(location: string)</code>, <code>pick_up(object_name: string)</code>, <code>place_down(object_name: string, location: string)</code>).</li>
<li class=""><strong>Context</strong>: Current robot state (e.g., <code>current_location: &quot;kitchen&quot;</code>), available objects, map information, or user preferences.</li>
<li class=""><strong>Output Format</strong>: A strict request for the output format, usually JSON or YAML, to make parsing easy.</li>
</ul>
<p><strong>Example Prompt Snippet:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&quot;You are a robotic assistant. Your task is to decompose a high-level user command into a sequence of atomic robot actions.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Available actions:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- move_to(location: string)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- find_object(object_name: string)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- pick_up(object_name: string)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- place_down(object_name: string, location: string)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Current Robot State: {{ current_robot_state_json_or_text }}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">User Command: &#x27;{{ user_command }}&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Provide your plan as a JSON list of actions. Example:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {&quot;action&quot;: &quot;move_to&quot;, &quot;location&quot;: &quot;living_room&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  {&quot;action&quot;: &quot;find_object&quot;, &quot;object_name&quot;: &quot;red_ball&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;</span><br></span></code></pre></div></div>
</li>
<li class="">
<p><strong>LLM Call</strong>: The node makes an API call to the LLM with the constructed prompt.</p>
</li>
<li class="">
<p><strong>Output Parsing</strong>: It parses the structured (e.g., JSON) response from the LLM, which contains the proposed sequence of actions.</p>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-action-execution-node-the-executor">3. Action Execution Node (The Executor)<a href="#3-action-execution-node-the-executor" class="hash-link" aria-label="Direct link to 3. Action Execution Node (The Executor)" title="Direct link to 3. Action Execution Node (The Executor)" translate="no">​</a></h3>
<p>This is another ROS 2 node that receives the structured action plan from the LLM Reasoning Node.</p>
<ul>
<li class=""><strong>Action Mapping</strong>: It maps the generic actions defined for the LLM (e.g., <code>pick_up</code>) to specific, low-level ROS 2 interfaces. This might involve:<!-- -->
<ul>
<li class="">Calling a <code>move_base_goal</code> action server (Nav2) for <code>move_to</code> commands.</li>
<li class="">Interfacing with a <code>MoveIt</code> action server for <code>pick_up</code>/<code>place_down</code> operations.</li>
<li class="">Subscribing to camera topics and calling object detection services for <code>find_object</code>.</li>
</ul>
</li>
<li class=""><strong>Execution Monitoring</strong>: It executes each action sequentially, monitors its success or failure, and reports the status. If an action fails, it can inform the LLM Reasoning Node, potentially triggering a replanning phase.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-workflow-clean-the-room">Example Workflow: &quot;Clean the Room&quot;<a href="#example-workflow-clean-the-room" class="hash-link" aria-label="Direct link to Example Workflow: &quot;Clean the Room&quot;" title="Direct link to Example Workflow: &quot;Clean the Room&quot;" translate="no">​</a></h2>
<ol>
<li class=""><strong>User says:</strong> &quot;Robot, please clean the room.&quot;</li>
<li class=""><strong>Whisper Node:</strong> Transcribes and publishes <code>&quot;Robot, please clean the room.&quot;</code> to <code>/voice_commands/text</code>.</li>
<li class=""><strong>LLM Reasoning Node:</strong> Subscribes to <code>/voice_commands/text</code>. Constructs a prompt, including current room state (e.g., &quot;desk has a mug and a book, floor has trash&quot;). Sends the prompt to the LLM.</li>
<li class=""><strong>LLM Output (Example JSON):</strong>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;move_to&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;desk&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;pick_up&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;object_name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;mug&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;move_to&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;shelf&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;place_down&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;object_name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;mug&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;shelf&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;move_to&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;floor&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;pick_up&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;object_name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;trash&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;move_to&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;trash_bin&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;action&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;place_down&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;object_name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;trash&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;trash_bin&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre></div></div>
</li>
<li class=""><strong>Action Execution Node:</strong> Receives this JSON list. It then calls the appropriate ROS 2 actions/services for <code>move_to</code>, <code>pick_up</code>, and <code>place_down</code>, passing the extracted <code>location</code> and <code>object_name</code> parameters.</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-and-future-directions">Challenges and Future Directions<a href="#challenges-and-future-directions" class="hash-link" aria-label="Direct link to Challenges and Future Directions" title="Direct link to Challenges and Future Directions" translate="no">​</a></h2>
<ul>
<li class=""><strong>Grounding</strong>: Ensuring the LLM&#x27;s plan is physically executable by the robot within its current capabilities and environment. An LLM might suggest an action the robot simply cannot perform.</li>
<li class=""><strong>Error Recovery and Replanning</strong>: What happens if an action fails (e.g., the robot drops the mug)? The system needs mechanisms for detecting failures and prompting the LLM to generate a revised plan.</li>
<li class=""><strong>Computational Cost and Latency</strong>: Running large LLMs, especially iteratively for replanning, can be slow and expensive. Techniques like local LLMs (running on powerful edge devices) or smaller, specialized models are being explored.</li>
<li class=""><strong>Safety and Ethics</strong>: Preventing the robot from performing unsafe or unethical actions, even if the LLM suggests them. This requires robust safety protocols and human oversight.</li>
<li class=""><strong>Multi-modal Input</strong>: Future systems will combine natural language with visual input (e.g., &quot;pick up <em>that</em> red mug&quot; while pointing).</li>
</ul>
<p>By harnessing the reasoning capabilities of LLMs, we are moving closer to robots that can understand and respond to the complex, nuanced world of human interaction, making them truly intelligent and versatile partners.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Mhammadkhan17/Physical-AI-Humanoid-Robotics-Course/tree/main/docs/module-4-vision-language-action/02-cognitive-planning-with-llms.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/voice-to-action-whisper"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Voice-to-Action with Whisper</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/capstone-project"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Capstone Project: The Autonomous Humanoid</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-role-of-llms-in-robotic-planning" class="table-of-contents__link toc-highlight">The Role of LLMs in Robotic Planning</a></li><li><a href="#the-cognitive-planning-pipeline-llm--ros-2" class="table-of-contents__link toc-highlight">The Cognitive Planning Pipeline (LLM + ROS 2)</a><ul><li><a href="#1-input-high-level-natural-language-command" class="table-of-contents__link toc-highlight">1. Input: High-Level Natural Language Command</a></li><li><a href="#2-llm-reasoning-node-the-planner" class="table-of-contents__link toc-highlight">2. LLM &quot;Reasoning&quot; Node (The Planner)</a></li><li><a href="#3-action-execution-node-the-executor" class="table-of-contents__link toc-highlight">3. Action Execution Node (The Executor)</a></li></ul></li><li><a href="#example-workflow-clean-the-room" class="table-of-contents__link toc-highlight">Example Workflow: &quot;Clean the Room&quot;</a></li><li><a href="#challenges-and-future-directions" class="table-of-contents__link toc-highlight">Challenges and Future Directions</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Course/docs/module-1-robotic-nervous-system/ros-nodes-topics-services">Modules Overview</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Course/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Mhammadkhan17/Physical-AI-Humanoid-Robotics-Course" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics. Built with Docusaurus.</div></div></div></footer> </div>
</body>
</html>