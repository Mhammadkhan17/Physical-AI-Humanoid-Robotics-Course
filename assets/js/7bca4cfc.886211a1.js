"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9374],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},9583:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vision-language-action/voice-to-action-whisper","title":"Voice-to-Action with Whisper","description":"The dream of naturally communicating with robots is becoming a reality. Moving beyond rigid graphical user interfaces, voice commands offer an intuitive and powerful way for humans to interact with intelligent machines. OpenAI\'s Whisper model stands at the forefront of this revolution, providing highly accurate and robust speech-to-text (STT) capabilities, even in challenging environments.","source":"@site/docs/module-4-vision-language-action/01-voice-to-action-whisper.mdx","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/voice-to-action-whisper","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/Mhammadkhan17/Physical-AI-Humanoid-Robotics-Course/tree/main/docs/module-4-vision-language-action/01-voice-to-action-whisper.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Voice-to-Action with Whisper"},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 Path Planning for Humanoids","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-robot-brain/nav2-path-planning"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms"}}');var o=i(4848),r=i(8453);const s={title:"Voice-to-Action with Whisper"},a=void 0,l={},c=[{value:"The Voice-to-Action Pipeline",id:"the-voice-to-action-pipeline",level:2},{value:"OpenAI Whisper: Speech-to-Text at its Best",id:"openai-whisper-speech-to-text-at-its-best",level:2},{value:"Key Capabilities:",id:"key-capabilities",level:3},{value:"Installation (Conceptual):",id:"installation-conceptual",level:3},{value:"Basic Python Usage:",id:"basic-python-usage",level:3},{value:"Integrating Whisper with ROS 2",id:"integrating-whisper-with-ros-2",level:2},{value:"Conceptual <code>WhisperNode</code> in Python:",id:"conceptual-whispernode-in-python",level:3},{value:"From Text to Action: Natural Language Understanding (NLU)",id:"from-text-to-action-natural-language-understanding-nlu",level:2},{value:"Challenges and Considerations",id:"challenges-and-considerations",level:2}];function d(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["The dream of naturally communicating with robots is becoming a reality. Moving beyond rigid graphical user interfaces, voice commands offer an intuitive and powerful way for humans to interact with intelligent machines. OpenAI's ",(0,o.jsx)(n.strong,{children:"Whisper"})," model stands at the forefront of this revolution, providing highly accurate and robust speech-to-text (STT) capabilities, even in challenging environments."]}),"\n",(0,o.jsx)(n.p,{children:'This chapter explores how to leverage Whisper to build a "Voice-to-Action" pipeline for controlling humanoid robots.'}),"\n",(0,o.jsx)(n.h2,{id:"the-voice-to-action-pipeline",children:"The Voice-to-Action Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The process of translating spoken commands into robot movements is a multi-stage pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Input"}),": A microphone captures raw audio data from the user."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech-to-Text (STT)"}),": The audio is fed into a Speech-to-Text model (like Whisper), which converts the spoken words into a written transcript."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),': The text transcript is then processed by an NLU system (often a Large Language Model or a specialized parser) to extract the user\'s intent (e.g., "move," "grasp") and any relevant parameters (e.g., "forward," "two meters," "red block").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Mapping"}),": The extracted intent and parameters are translated into a specific, executable robot action or sequence of actions (e.g., publishing a ",(0,o.jsx)(n.code,{children:"Twist"})," message to move, calling a ",(0,o.jsx)(n.code,{children:"pick_object"})," service)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Execution"}),": The robot's controllers receive these actions and perform the desired task."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-speech-to-text-at-its-best",children:"OpenAI Whisper: Speech-to-Text at its Best"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper is an open-source, general-purpose speech recognition model. It's trained on a massive dataset of diverse audio, making it remarkably robust to accents, background noise, and technical jargon."}),"\n",(0,o.jsx)(n.h3,{id:"key-capabilities",children:"Key Capabilities:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High Accuracy"}),": Delivers state-of-the-art performance across many domains."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual"}),": Can transcribe and translate speech in many different languages."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Performs well even in noisy environments."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"installation-conceptual",children:"Installation (Conceptual):"}),"\n",(0,o.jsx)(n.p,{children:"You can typically install Whisper via pip:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,o.jsx)(n.h3,{id:"basic-python-usage",children:"Basic Python Usage:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import whisper\r\n\r\n# Load one of the Whisper models (e.g., 'tiny', 'base', 'small', 'medium', 'large')\r\n# 'base' is a good balance of speed and accuracy for many applications.\r\nmodel = whisper.load_model(\"base\")\r\n\r\n# Transcribe an audio file\r\naudio_file_path = \"path/to/your/audio.mp3\" # Replace with actual audio file\r\nresult = model.transcribe(audio_file_path)\r\n\r\n# Print the transcribed text\r\nprint(result[\"text\"])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-whisper-with-ros-2",children:"Integrating Whisper with ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"To integrate Whisper into a ROS 2 robot, you would typically create a dedicated ROS 2 node that handles audio input, processes it with Whisper, and publishes the resulting text to a ROS topic."}),"\n",(0,o.jsxs)(n.h3,{id:"conceptual-whispernode-in-python",children:["Conceptual ",(0,o.jsx)(n.code,{children:"WhisperNode"})," in Python:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String # To publish the transcribed text\r\n# For actual audio input, you would use a ROS 2 package that\r\n# captures microphone audio and publishes it as a ROS message.\r\n# Example: from audio_common_msgs.msg import AudioData # If using audio_common\r\nimport whisper\r\nimport numpy as np # For potential audio data manipulation\r\n\r\nclass WhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__('whisper_node')\r\n        self.model = whisper.load_model(\"base\") # Load the Whisper model\r\n        \r\n        # Publisher for the transcribed text\r\n        self.text_publisher = self.create_publisher(String, '/voice_commands/text', 10)\r\n        \r\n        # Conceptual subscriber for incoming audio data from a microphone source\r\n        # You would replace AudioData with the actual message type your audio capture node uses\r\n        # self.audio_subscription = self.create_subscription(\r\n        #     AudioData,\r\n        #     '/audio_input', # Topic where raw audio data is published\r\n        #     self.audio_callback,\r\n        #     10\r\n        # )\r\n        self.get_logger().info('Whisper Node initialized and ready for audio input.')\r\n\r\n    # This callback would process incoming audio chunks\r\n    # For a real system, you'd need logic to buffer audio, detect speech, etc.\r\n    # def audio_callback(self, msg: AudioData):\r\n    #     # Example: Convert raw audio bytes to a format Whisper can process\r\n    #     # This part is highly dependent on your AudioData message format\r\n    #     audio_np = np.frombuffer(msg.data, dtype=np.int16).flatten().astype(np.float32) / 32768.0\r\n    #     \r\n    #     # Perform transcription\r\n    #     # You might want to save to a temporary file or use an in-memory buffer\r\n    #     result = self.model.transcribe(audio_np, fp16=False) # fp16=False for CPU or non-NVIDIA GPUs\r\n    #     \r\n    #     transcribed_text = result[\"text\"].strip()\r\n    #     if transcribed_text:\r\n    #         self.publish_text(transcribed_text)\r\n\r\n    def publish_text(self, text_to_publish: str):\r\n        msg = String()\r\n        msg.data = text_to_publish\r\n        self.text_publisher.publish(msg)\r\n        self.get_logger().info(f'Whisper published: \"{text_to_publish}\"')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    whisper_node = WhisperNode()\r\n    \r\n    # In a full application, rclpy.spin would handle callbacks\r\n    # rclpy.spin(whisper_node)\r\n    \r\n    # For demonstration, simulate a command being transcribed\r\n    print(\"\\n--- Simulating a voice command ---\")\r\n    # You would typically have an audio recording here, e.g., using PyAudio\r\n    # For this example, let's just pretend we transcribed \"Robot, move forward two meters\"\r\n    simulated_transcript = \"Robot, move forward two meters.\"\r\n    whisper_node.publish_text(simulated_transcript)\r\n    \r\n    whisper_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"from-text-to-action-natural-language-understanding-nlu",children:"From Text to Action: Natural Language Understanding (NLU)"}),"\n",(0,o.jsxs)(n.p,{children:["Once Whisper provides the text transcript (e.g., published on ",(0,o.jsx)(n.code,{children:"/voice_commands/text"}),"), another ROS 2 node takes over. This NLU node is responsible for:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent Recognition"}),": Determining the user's primary goal (e.g., ",(0,o.jsx)(n.code,{children:"move"}),", ",(0,o.jsx)(n.code,{children:"pickup"}),", ",(0,o.jsx)(n.code,{children:"explore"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Entity Extraction"}),': Identifying key parameters within the command (e.g., for "move forward two meters," extract ',(0,o.jsx)(n.code,{children:"distance=2"}),", ",(0,o.jsx)(n.code,{children:"unit=meters"}),", ",(0,o.jsx)(n.code,{children:"direction=forward"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Mapping"}),": Translating the recognized intent and extracted entities into specific ROS 2 actions or service calls. This might involve generating a ",(0,o.jsx)(n.code,{children:"geometry_msgs/Twist"})," message for movement, calling a ",(0,o.jsx)(n.code,{children:"move_base_goal"})," action, or invoking a ",(0,o.jsx)(n.code,{children:"pick_object"})," service."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"challenges-and-considerations",children:"Challenges and Considerations"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency"}),": For seamless HRI, the entire pipeline from speech to robot action must be low-latency. GPU-accelerated processing and efficient communication are vital."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy & Robustness"}),": Even Whisper can make mistakes. The NLU layer should be robust to slight transcription errors. Training Whisper on domain-specific audio can improve performance."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Wake Word Detection"}),': To avoid continuous resource consumption and unwanted actions, a separate, lightweight model is often used to detect a "wake word" (e.g., "Hey Robot," "Computer") that activates the main Whisper transcription system.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Security & Privacy"}),": Handling voice data, especially in real-world deployments, requires careful consideration of security and user privacy."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"By integrating Whisper, you can unlock a more natural and intuitive mode of interaction with your humanoid robot, making it more accessible and responsive to human commands."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);