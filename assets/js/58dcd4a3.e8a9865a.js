"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[7623],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},9440:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2-digital-twin/rendering-in-unity","title":"High-Fidelity Rendering in Unity","description":"While Gazebo provides a robust and accurate physics simulation, the world of Physical AI often requires a level of visual realism that can be challenging to achieve in traditional robotics simulators. This is where Unity, a professional real-time 3D development platform, comes in. Unity excels at creating photorealistic graphics and rich, interactive environments, making it an invaluable tool for both training AI perception models and developing intuitive Human-Robot Interaction (HRI) scenarios.","source":"@site/docs/module-2-digital-twin/02-rendering-in-unity.mdx","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/rendering-in-unity","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/rendering-in-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/Mhammadkhan17/Physical-AI-Humanoid-Robotics-Course/tree/main/docs/module-2-digital-twin/02-rendering-in-unity.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"High-Fidelity Rendering in Unity"},"sidebar":"tutorialSidebar","previous":{"title":"Simulating Physics in Gazebo","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulating-physics-in-gazebo"},"next":{"title":"Simulating Sensors","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulating-sensors"}}');var o=i(4848),s=i(8453);const r={title:"High-Fidelity Rendering in Unity"},a=void 0,l={},c=[{value:"Connecting ROS 2 and Unity",id:"connecting-ros-2-and-unity",level:2},{value:"High-Fidelity Rendering for Synthetic Data",id:"high-fidelity-rendering-for-synthetic-data",level:2},{value:"Building Interactive Scenarios",id:"building-interactive-scenarios",level:2},{value:"Example HRI Scenario:",id:"example-hri-scenario",level:4},{value:"Gazebo vs. Unity: The Right Tool for the Job",id:"gazebo-vs-unity-the-right-tool-for-the-job",level:2}];function d(e){const n={code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"While Gazebo provides a robust and accurate physics simulation, the world of Physical AI often requires a level of visual realism that can be challenging to achieve in traditional robotics simulators. This is where Unity, a professional real-time 3D development platform, comes in. Unity excels at creating photorealistic graphics and rich, interactive environments, making it an invaluable tool for both training AI perception models and developing intuitive Human-Robot Interaction (HRI) scenarios."}),"\n",(0,o.jsx)(n.p,{children:"This chapter explores how to connect Unity to ROS 2 and leverage its powerful rendering engine to build a visually stunning digital twin."}),"\n",(0,o.jsx)(n.h2,{id:"connecting-ros-2-and-unity",children:"Connecting ROS 2 and Unity"}),"\n",(0,o.jsxs)(n.p,{children:["To bridge the gap between the ROS 2 ecosystem and the Unity game engine, we use the official ",(0,o.jsx)(n.strong,{children:"ROS-TCP-Connector"})," package provided by Unity. This package allows Unity to communicate with a ROS 2 network over a standard TCP/IP socket."]}),"\n",(0,o.jsx)(n.p,{children:"The architecture consists of two key parts:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS-TCP-Endpoint"}),": This is a ROS 2 node that runs on the ROS side. It acts as a server, listening for connections from Unity and translating incoming/outgoing messages between ROS formats and a generic JSON format."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsxs)(n.strong,{children:[(0,o.jsx)(n.code,{children:"ROSConnection"})," Prefab/Singleton"]}),": On the Unity side, this component manages the connection to the ROS Endpoint. You configure it with the IP address and port of your ROS machine, and it handles the serialization and deserialization of messages."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This setup allows your Unity application to subscribe to ROS topics, publish messages, and call services, just like any other ROS node."}),"\n",(0,o.jsx)(n.h2,{id:"high-fidelity-rendering-for-synthetic-data",children:"High-Fidelity Rendering for Synthetic Data"}),"\n",(0,o.jsxs)(n.p,{children:["Unity's power lies in its advanced graphics capabilities, particularly the ",(0,o.jsx)(n.strong,{children:"High Definition Render Pipeline (HDRP)"}),". While Gazebo is excellent for physics, HDRP is designed to produce photorealistic visuals."]}),"\n",(0,o.jsxs)(n.p,{children:["This is critical for a key task in modern robotics: generating ",(0,o.jsx)(n.strong,{children:"synthetic data"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Why is Synthetic Data Important?"}),"\r\nTraining a deep learning model for computer vision requires thousands, or even millions, of labeled images. Acquiring and labeling this data from the real world is incredibly expensive and time-consuming. A high-fidelity simulator allows us to generate this data automatically. We can render a target object (like a specific soda can) in thousands of different lighting conditions, positions, and orientations, all with perfect, automatically-generated labels (like bounding boxes or segmentation masks)."]}),"\n",(0,o.jsx)(n.p,{children:"Key HDRP features for generating synthetic data include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Physically-Based Rendering (PBR) Materials"}),": Create materials that accurately mimic how they reflect light in the real world."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Advanced Lighting"}),": Use techniques like real-time Ray Tracing and Global Illumination to create incredibly realistic shadows and reflections."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Post-Processing Effects"}),": Add cinematic camera effects like depth of field, bloom, and color grading to make the final image more closely resemble a real camera feed."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["An AI perception model trained on this high-quality synthetic data has a much better chance of performing well when deployed on a real robot\u2014a concept known as ",(0,o.jsx)(n.strong,{children:"sim-to-real transfer"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"building-interactive-scenarios",children:"Building Interactive Scenarios"}),"\n",(0,o.jsx)(n.p,{children:"Beyond just looking good, Unity's game engine architecture makes it perfect for building interactive environments for Human-Robot Interaction (HRI) studies or intuitive robot commanding."}),"\n",(0,o.jsx)(n.h4,{id:"example-hri-scenario",children:"Example HRI Scenario:"}),"\n",(0,o.jsx)(n.p,{children:"Imagine a virtual apartment where you want to command a humanoid robot to fetch a drink."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"User Interaction"}),": The user, viewing the scene from a first-person perspective, clicks on an object (e.g., a soda can)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Unity Script (C#)"}),": A C# script attached to the camera detects the mouse click and performs a raycast into the 3D scene to identify the clicked object."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS Publication"}),": Upon identifying the soda can, the script gets its 3D coordinates. It then creates a ROS ",(0,o.jsx)(n.code,{children:"PoseStamped"})," message (a standard message type for representing a position and orientation in space) and ",(0,o.jsx)(n.strong,{children:"publishes"})," it to a topic, for example, ",(0,o.jsx)(n.code,{children:"/move_to_goal"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Brain (ROS)"}),": The robot's AI or navigation stack, subscribed to ",(0,o.jsx)(n.code,{children:"/move_to_goal"}),", receives the message. This triggers its path planning and manipulation routines to navigate to the table and pick up the can."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Here\u2019s a conceptual C# script for the Unity side:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\r\nusing Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Geometry;\r\n\r\npublic class ClickCommander : MonoBehaviour\r\n{\r\n    ROSConnection ros;\r\n\r\n    void Start()\r\n    {\r\n        // Get the ROSConnection instance\r\n        ros = ROSConnection.GetOrCreateInstance();\r\n        // Register the topic we want to publish to\r\n        ros.RegisterPublisher<PoseStampedMsg>("/move_to_goal");\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        if (Input.GetMouseButtonDown(0)) // On left mouse click\r\n        {\r\n            Ray ray = Camera.main.ScreenPointToRay(Input.mousePosition);\r\n            if (Physics.Raycast(ray, out RaycastHit hit))\r\n            {\r\n                if (hit.collider.gameObject.CompareTag("Interactable"))\r\n                {\r\n                    // Create the ROS message\r\n                    PoseStampedMsg goalMsg = new PoseStampedMsg();\r\n                    goalMsg.header.frame_id = "world";\r\n                    goalMsg.pose.position = hit.point;\r\n                    \r\n                    // Publish the message\r\n                    ros.Publish("/move_to_goal", goalMsg);\r\n\r\n                    Debug.Log("Published goal to /move_to_goal: " + hit.point);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"gazebo-vs-unity-the-right-tool-for-the-job",children:"Gazebo vs. Unity: The Right Tool for the Job"}),"\n",(0,o.jsx)(n.p,{children:"It's helpful to think of Gazebo and Unity as complementary tools, not competitors."}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Feature"}),(0,o.jsx)(n.th,{children:"Gazebo"}),(0,o.jsx)(n.th,{children:"Unity"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Primary Strength"})}),(0,o.jsxs)(n.td,{children:["Fast and accurate ",(0,o.jsx)(n.strong,{children:"physics"})," simulation."]}),(0,o.jsxs)(n.td,{children:["Photorealistic ",(0,o.jsx)(n.strong,{children:"rendering"})," and interactivity."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Typical Use Case"})}),(0,o.jsx)(n.td,{children:"Algorithm validation, control system design."}),(0,o.jsx)(n.td,{children:"Synthetic data generation, HRI, VR/AR apps."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"ROS Integration"})}),(0,o.jsx)(n.td,{children:"Native, deep, and direct."}),(0,o.jsx)(n.td,{children:"Good, via the ROS-TCP-Connector package."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Metaphor"})}),(0,o.jsx)(n.td,{children:"The Scientist's Lab"}),(0,o.jsx)(n.td,{children:"The Hollywood Movie Set"})]})]})]}),"\n",(0,o.jsx)(n.p,{children:"In an ideal workflow, you might use Gazebo to validate your robot's core dynamics and control loops, then bring your robot into a high-fidelity Unity environment to train its perception system and design the user-facing application."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);