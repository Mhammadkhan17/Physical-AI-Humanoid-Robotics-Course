"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[336],{3788:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-2-digital-twin/simulating-sensors","title":"Simulating Sensors","description":"A robot without sensors is blind, deaf, and dumb. To create an intelligent digital twin, we must give our simulated robot the ability to \\"see\\" and \\"feel\\" its virtual environment. This is achieved by simulating sensors. Accurate sensor simulation is critical for developing and testing the perception, localization, and navigation algorithms that form the foundation of a robot\'s autonomy.","source":"@site/docs/module-2-digital-twin/03-simulating-sensors.mdx","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/simulating-sensors","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulating-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/Mhammadkhan17/Physical-AI-Humanoid-Robotics-Course/tree/main/docs/module-2-digital-twin/03-simulating-sensors.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Simulating Sensors"},"sidebar":"tutorialSidebar","previous":{"title":"High-Fidelity Rendering in Unity","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/rendering-in-unity"},"next":{"title":"NVIDIA Isaac Sim","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-robot-brain/nvidia-isaac-sim"}}');var r=i(4848),s=i(8453);const t={title:"Simulating Sensors"},o=void 0,l={},d=[{value:"Simulating a 2D LiDAR",id:"simulating-a-2d-lidar",level:2},{value:"Example: Adding a LiDAR to a URDF",id:"example-adding-a-lidar-to-a-urdf",level:3},{value:"Simulating a Depth Camera",id:"simulating-a-depth-camera",level:2},{value:"Example: Adding a Depth Camera",id:"example-adding-a-depth-camera",level:3},{value:"Simulating an IMU",id:"simulating-an-imu",level:2},{value:"Example: Adding an IMU",id:"example-adding-an-imu",level:3}];function c(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:'A robot without sensors is blind, deaf, and dumb. To create an intelligent digital twin, we must give our simulated robot the ability to "see" and "feel" its virtual environment. This is achieved by simulating sensors. Accurate sensor simulation is critical for developing and testing the perception, localization, and navigation algorithms that form the foundation of a robot\'s autonomy.'}),"\n",(0,r.jsxs)(n.p,{children:["In Gazebo, sensors are not magical entities; they are implemented using ",(0,r.jsx)(n.strong,{children:"Plugins"}),". A plugin is a self-contained, compiled library that can be attached to a link in your robot's model (via its URDF or SDF file) to add new functionality, such as simulating a specific type of sensor."]}),"\n",(0,r.jsx)(n.p,{children:"This chapter covers how to simulate three of the most common sensors in robotics: LiDARs, depth cameras, and IMUs."}),"\n",(0,r.jsx)(n.h2,{id:"simulating-a-2d-lidar",children:"Simulating a 2D LiDAR"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"})," sensor is the workhorse of 2D navigation and mapping. It spins a laser beam in a plane, measuring the distance to objects at many points around the robot."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo Plugin"}),": ",(0,r.jsx)(n.code,{children:"libgazebo_ros_laser.so"})," (the exact name may vary slightly between ROS/Gazebo versions, e.g., ",(0,r.jsx)(n.code,{children:"libgazebo_ros_ray_sensor.so"}),'). This plugin casts a specified number of "rays" into the simulated world and reports the distance at which they hit an object.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS Output"}),": The plugin publishes ",(0,r.jsx)(n.code,{children:"sensor_msgs/LaserScan"})," messages on a ROS topic. This message is an array of distance measurements (ranges) for each ray."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-adding-a-lidar-to-a-urdf",children:"Example: Adding a LiDAR to a URDF"}),"\n",(0,r.jsxs)(n.p,{children:["To add a LiDAR, you create a new link for the sensor and attach it to your robot's ",(0,r.jsx)(n.code,{children:"base_link"})," with a ",(0,r.jsx)(n.code,{children:"fixed"})," joint. Then, you add the ",(0,r.jsx)(n.code,{children:"<gazebo>"})," tag with a ",(0,r.jsx)(n.code,{children:"<sensor>"})," block."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- A link for our LiDAR sensor --\x3e\r\n<link name="laser_link"></link>\r\n\r\n<joint name="laser_joint" type="fixed">\r\n  <parent link="base_link"/>\r\n  <child link="laser_link"/>\r\n  <origin xyz="0.1 0 0.2" rpy="0 0 0"/>\r\n</joint>\r\n\r\n\x3c!-- Gazebo plugin for the LiDAR --\x3e\r\n<gazebo reference="laser_link">\r\n  <sensor type="ray" name="hokuyo_laser">\r\n    <pose>0 0 0 0 0 0</pose>\r\n    <visualize>true</visualize>  \x3c!-- Show the laser beams in the GUI --\x3e\r\n    <update_rate>20.0</update_rate> \x3c!-- Publish at 20 Hz --\x3e\r\n    <ray>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>720</samples>\r\n          <resolution>1</resolution>\r\n          <min_angle>-1.5708</min_angle>\r\n          <max_angle>1.5708</max_angle>\r\n        </horizontal>\r\n      </scan>\r\n      <range>\r\n        <min>0.10</min>\r\n        <max>10.0</max>\r\n        <resolution>0.01</resolution>\r\n      </range>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.01</stddev>\r\n      </noise>\r\n    </ray>\r\n    <plugin name="gazebo_ros_laser_controller" filename="libgazebo_ros_laser.so">\r\n      <topicName>/scan</topicName>\r\n      <frameName>laser_link</frameName>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Key Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"<samples>"}),": The number of laser beams to simulate per scan."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"<min_angle>"}),"/",(0,r.jsx)(n.code,{children:"<max_angle>"}),": The start and end angle of the scan (in radians)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"<range>"}),": The minimum and maximum distance the laser can detect."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"<noise>"}),": ",(0,r.jsx)(n.strong,{children:"Crucially important for realism"}),". Real sensors are noisy. Adding a small amount of Gaussian noise makes your simulation a much better approximation of reality."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"simulating-a-depth-camera",children:"Simulating a Depth Camera"}),"\n",(0,r.jsxs)(n.p,{children:["A depth camera (like an Intel RealSense or Microsoft Kinect) is a powerful sensor that provides two types of images: a standard color (RGB) image and a ",(0,r.jsx)(n.strong,{children:"depth image"}),", where each pixel's value corresponds to its distance from the camera. This is essential for 3D perception and obstacle avoidance."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo Plugin"}),": ",(0,r.jsx)(n.code,{children:"libgazebo_ros_camera.so"})," or the more specific ",(0,r.jsx)(n.code,{children:"libgazebo_ros_depth_camera.so"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS Output"}),": This plugin can publish multiple topics:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," for the RGB color image."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," for the depth image."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})," for a 3D point cloud constructed from the depth data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})," with the camera's intrinsic calibration parameters."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-adding-a-depth-camera",children:"Example: Adding a Depth Camera"}),"\n",(0,r.jsx)(n.p,{children:"The setup is similar to the LiDAR, attaching a sensor to a link."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="camera_link">\r\n  <sensor type="depth" name="realsense_camera">\r\n    <update_rate>30.0</update_rate>\r\n    <camera name="head">\r\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- ~60 degrees --\x3e\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.1</near>\r\n        <far>100</far>\r\n      </clip>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.007</stddev>\r\n      </noise>\r\n    </camera>\r\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n      <frame_name>camera_link_optical</frame_name>\r\n      \x3c!-- Topic names can be remapped --\x3e\r\n      <camera_name>realsense</camera_name>\r\n      <image_topic_name>rgb/image_raw</image_topic_name>\r\n      <camera_info_topic_name>rgb/camera_info</camera_info_topic_name>\r\n      <depth_image_topic_name>depth/image_raw</depth_image_topic_name>\r\n      <depth_info_topic_name>depth/camera_info</depth_info_topic_name>\r\n      <point_cloud_topic_name>depth/points</point_cloud_topic_name>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"simulating-an-imu",children:"Simulating an IMU"}),"\n",(0,r.jsxs)(n.p,{children:["An ",(0,r.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit)"})," is the robot's sense of balance. It measures orientation, angular velocity, and linear acceleration, which is vital for stabilizing the robot and estimating its state."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo Plugin"}),": ",(0,r.jsx)(n.code,{children:"libgazebo_ros_imu_sensor.so"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS Output"}),": Publishes ",(0,r.jsx)(n.code,{children:"sensor_msgs/Imu"})," messages, which contain a ",(0,r.jsx)(n.code,{children:"Quaternion"})," for orientation, and ",(0,r.jsx)(n.code,{children:"Vector3"})," messages for angular velocity and linear acceleration."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-adding-an-imu",children:"Example: Adding an IMU"}),"\n",(0,r.jsx)(n.p,{children:"Real IMUs are notoriously noisy and suffer from drift. Simulating this noise is absolutely critical for testing any localization or state estimation algorithm (like an Extended Kalman Filter)."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="imu_link">\r\n  <sensor name="my_imu" type="imu">\r\n    <always_on>true</always_on>\r\n    <update_rate>100</update_rate>\r\n    <imu>\r\n      <angular_velocity>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>2e-4</stddev>\r\n          </noise>\r\n        </x>\r\n        \x3c!-- Noise for Y and Z axes --\x3e\r\n      </angular_velocity>\r\n      <linear_acceleration>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>1.7e-2</stddev>\r\n          </noise>\r\n        </x>\r\n        \x3c!-- Noise for Y and Z axes --\x3e\r\n      </linear_acceleration>\r\n    </imu>\r\n    <plugin filename="libgazebo_ros_imu_sensor.so" name="imu_plugin">\r\n      <topicName>imu</topicName>\r\n      <frameName>imu_link</frameName>\r\n      <updateRateHZ>100.0</updateRateHZ>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.p,{children:"By leveraging Gazebo's powerful sensor plugins, you can create a simulated robot that perceives its environment in a way that closely mimics a real-world robot. This allows you to develop and debug complex autonomy software with confidence before ever touching physical hardware."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var a=i(6540);const r={},s=a.createContext(r);function t(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);