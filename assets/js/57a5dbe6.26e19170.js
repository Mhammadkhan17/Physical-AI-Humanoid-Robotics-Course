"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4318],{5640:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-1-robotic-nervous-system/ros-nodes-topics-services","label":"ROS 2 Nodes, Topics, and Services","docId":"module-1-robotic-nervous-system/ros-nodes-topics-services","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-1-robotic-nervous-system/python-rclpy-bridge","label":"Bridging Python Agents to ROS Controllers","docId":"module-1-robotic-nervous-system/python-rclpy-bridge","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-1-robotic-nervous-system/urdf-for-humanoids","label":"Understanding URDF for Humanoids","docId":"module-1-robotic-nervous-system/urdf-for-humanoids","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulating-physics-in-gazebo","label":"Simulating Physics in Gazebo","docId":"module-2-digital-twin/simulating-physics-in-gazebo","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/rendering-in-unity","label":"High-Fidelity Rendering in Unity","docId":"module-2-digital-twin/rendering-in-unity","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulating-sensors","label":"Simulating Sensors","docId":"module-2-digital-twin/simulating-sensors","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-robot-brain/nvidia-isaac-sim","label":"NVIDIA Isaac Sim","docId":"module-3-ai-robot-brain/nvidia-isaac-sim","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-robot-brain/isaac-ros-vslam","label":"Isaac ROS: Hardware-Accelerated VSLAM","docId":"module-3-ai-robot-brain/isaac-ros-vslam","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-robot-brain/nav2-path-planning","label":"Nav2 Path Planning for Humanoids","docId":"module-3-ai-robot-brain/nav2-path-planning","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/voice-to-action-whisper","label":"Voice-to-Action with Whisper","docId":"module-4-vision-language-action/voice-to-action-whisper","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/cognitive-planning-with-llms","label":"Cognitive Planning with LLMs","docId":"module-4-vision-language-action/cognitive-planning-with-llms","unlisted":false},{"type":"link","href":"/Physical-AI-Humanoid-Robotics-Course/docs/module-4-vision-language-action/capstone-project","label":"Capstone Project: The Autonomous Humanoid","docId":"module-4-vision-language-action/capstone-project","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"module-1-robotic-nervous-system/python-rclpy-bridge":{"id":"module-1-robotic-nervous-system/python-rclpy-bridge","title":"Bridging Python Agents to ROS Controllers","description":"In modern robotics, the \\"brain\\" of a robot\u2014often a complex AI or machine learning model\u2014is typically developed in Python. However, the robot\'s \\"body\\"\u2014its motors, sensors, and low-level controllers\u2014runs on the robust, real-time ROS 2 framework. The critical challenge is making them talk to each other. This chapter explains how to build a \\"bridge\\" using rclpy, the official ROS 2 Python client library, to connect your intelligent Python agent to the ROS 2 world.","sidebar":"tutorialSidebar"},"module-1-robotic-nervous-system/ros-nodes-topics-services":{"id":"module-1-robotic-nervous-system/ros-nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"Welcome to the nervous system of your robot! In the world of Physical AI, ROS 2 (Robot Operating System 2) acts as the essential middleware, allowing different parts of your robot\'s software to communicate and work together seamlessly. Think of it as the central nervous system that carries signals between the brain (your AI code) and the body (the robot\'s motors and sensors).","sidebar":"tutorialSidebar"},"module-1-robotic-nervous-system/urdf-for-humanoids":{"id":"module-1-robotic-nervous-system/urdf-for-humanoids","title":"Understanding URDF for Humanoids","description":"Every robot, whether in a simulation or the real world, needs a \\"blueprint.\\" In the ROS ecosystem, this blueprint is the Unified Robot Description Format (URDF). It\'s an XML-based file that precisely describes the robot\'s physical structure, from its limbs and joints to its visual appearance and physical properties. For humanoids, a well-structured URDF is the essential foundation for everything from basic visualization to complex motion planning.","sidebar":"tutorialSidebar"},"module-2-digital-twin/rendering-in-unity":{"id":"module-2-digital-twin/rendering-in-unity","title":"High-Fidelity Rendering in Unity","description":"While Gazebo provides a robust and accurate physics simulation, the world of Physical AI often requires a level of visual realism that can be challenging to achieve in traditional robotics simulators. This is where Unity, a professional real-time 3D development platform, comes in. Unity excels at creating photorealistic graphics and rich, interactive environments, making it an invaluable tool for both training AI perception models and developing intuitive Human-Robot Interaction (HRI) scenarios.","sidebar":"tutorialSidebar"},"module-2-digital-twin/simulating-physics-in-gazebo":{"id":"module-2-digital-twin/simulating-physics-in-gazebo","title":"Simulating Physics in Gazebo","description":"Before a robot can walk, it must learn to fall. Welcome to Gazebo, a powerful 3D robotics simulator that allows us to safely and accurately test our robots in a virtual environment. For Physical AI, simulation is not just a convenience\u2014it\'s a necessity. It allows us to iterate on our designs, train our AI models, and test control algorithms thousands of times in a fraction of the time and cost it would take with a real robot.","sidebar":"tutorialSidebar"},"module-2-digital-twin/simulating-sensors":{"id":"module-2-digital-twin/simulating-sensors","title":"Simulating Sensors","description":"A robot without sensors is blind, deaf, and dumb. To create an intelligent digital twin, we must give our simulated robot the ability to \\"see\\" and \\"feel\\" its virtual environment. This is achieved by simulating sensors. Accurate sensor simulation is critical for developing and testing the perception, localization, and navigation algorithms that form the foundation of a robot\'s autonomy.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/isaac-ros-vslam":{"id":"module-3-ai-robot-brain/isaac-ros-vslam","title":"Isaac ROS: Hardware-Accelerated VSLAM","description":"While Isaac Sim provides the perfect virtual training ground for a robot\'s AI, NVIDIA Isaac ROS provides the high-performance engine to run that AI in the real world. Isaac ROS is not a simulator; it is a collection of hardware-accelerated ROS 2 packages designed to give robots real-time, high-performance perception capabilities, especially on NVIDIA Jetson platforms.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/nav2-path-planning":{"id":"module-3-ai-robot-brain/nav2-path-planning","title":"Nav2 Path Planning for Humanoids","description":"For a robot to move autonomously from one point to another, it needs a robust navigation system. In ROS 2, the standard solution is Nav2. Nav2 is a powerful, configurable navigation stack designed to get a robot to a desired goal pose safely and efficiently, while avoiding obstacles. While Nav2 excels with wheeled robots, adapting it for a bipedal humanoid requires special considerations due to humanoids\' unique locomotion capabilities and constraints.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/nvidia-isaac-sim":{"id":"module-3-ai-robot-brain/nvidia-isaac-sim","title":"NVIDIA Isaac Sim","description":"Welcome to the cutting edge of robotics simulation. While tools like Gazebo and Unity are powerful, the modern AI-driven robotics stack requires a simulator built from the ground up for photorealism, physics fidelity, and seamless integration with AI training workflows. Enter NVIDIA Isaac Sim.","sidebar":"tutorialSidebar"},"module-4-vision-language-action/capstone-project":{"id":"module-4-vision-language-action/capstone-project","title":"Capstone Project: The Autonomous Humanoid","description":"This capstone project is the culmination of your journey through Physical AI. It challenges you to integrate all the concepts, tools, and technologies learned throughout this course\u2014from ROS 2 fundamentals and advanced simulation to hardware-accelerated perception and AI-driven cognitive planning\u2014into a single, autonomous humanoid robot system.","sidebar":"tutorialSidebar"},"module-4-vision-language-action/cognitive-planning-with-llms":{"id":"module-4-vision-language-action/cognitive-planning-with-llms","title":"Cognitive Planning with LLMs","description":"Imagine telling a robot, \\"Please clean the room,\\" and it understands, plans, and executes the necessary steps. This is the promise of Cognitive Planning with Large Language Models (LLMs). It bridges the vast gap between high-level human intent expressed in natural language and the precise, atomic actions a robot must perform. LLMs are not just for generating text; they are becoming powerful reasoning engines for robotic control.","sidebar":"tutorialSidebar"},"module-4-vision-language-action/voice-to-action-whisper":{"id":"module-4-vision-language-action/voice-to-action-whisper","title":"Voice-to-Action with Whisper","description":"The dream of naturally communicating with robots is becoming a reality. Moving beyond rigid graphical user interfaces, voice commands offer an intuitive and powerful way for humans to interact with intelligent machines. OpenAI\'s Whisper model stands at the forefront of this revolution, providing highly accurate and robust speech-to-text (STT) capabilities, even in challenging environments.","sidebar":"tutorialSidebar"}}}}')}}]);